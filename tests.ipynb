{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afonso-tiago/thesis-notebooks/blob/main/tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOgI-RlNEwnz"
      },
      "source": [
        "# Intoduction \n",
        "\n",
        "This is one of the complementary notebooks to the bachelor thesis titled \"*Comparing Performance of Different Goal Functionals in Solving PDEs Using Neural Networks*\". It contains the complete code to perform every test described in the thesis and is written in such a way to allow easy construction of own new tests.\n",
        "\n",
        "> A good way to navigate Google Colaboratory notebooks is by using the built-in table of contents; the first item in the menu bar on the left side of the screen.\n",
        "\n",
        "> Helpful keyboard shortcuts are: \n",
        "* <kbd>Shift</kbd> + <kbd>Enter</kbd>: executes a cell and jump to the next one\n",
        "* <kbd>Ctrl</kbd> + <kbd>Enter</kbd>: executes a cell but stays in that cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v67UjWCNH7dn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily enable hardware acceleration by navigating to `Edit`\n",
        "and clicking on the `Notebook Settings`. This should open a popup where we can select a hardware accelerator. Selecting None or GPU will work immediately without adjustments to the code. If we want to use TPUs we need make some smaller changes as TPUs are usually used for distributed computing inside a cluster.\n"
      ],
      "metadata": {
        "id": "Rd8enWkZf8QD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell tests which GPUs are connected to our notebook"
      ],
      "metadata": {
        "id": "i9861Eb-kfQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "eqSIb_LB32M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A list of all necessary imports"
      ],
      "metadata": {
        "id": "5zBl73pOlAqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iVIt80ZH7EI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from scipy.stats import qmc\n",
        "# !pip install scipy --upgrade\n",
        "\n",
        "from IPython.display import clear_output as clear\n",
        "\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from os import listdir\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "log_base_route = \"logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to save log files permanently you can connect your google drive by running this cell. **Otherwise just skip this cell.**"
      ],
      "metadata": {
        "id": "QyVhTHhnl4EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# adjust the base route of the log files such that they are directly saved to the drive\n",
        "log_base_route = \"/content/drive/MyDrive/PINN_vs_DRM_tests/logs\""
      ],
      "metadata": {
        "id": "Z3zc9WQ9mL4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qKM8HzcHm-f"
      },
      "source": [
        "# Interior and Boundary Point Generation\n",
        "In this section we will construct all necessary generators\n",
        "and create right-hand sides and exact solutions corresponding to different settings of the Poisson problem. The theoretical counterpart to this is **section 2.3** in the thesis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf9l6p53gVGR"
      },
      "source": [
        "## Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell contains various generators for the interior and boundary of different domains.\n",
        "\n",
        "*   interior of hypercube randomly\n",
        "*   interior of hypercube via LHS\n",
        "*   interior of n-sphere randomly\n",
        "*   interior of pie shaped domain randomly\n",
        "*   boundary of hypercube regularly\n",
        "*   boundary of hypercube randomly\n",
        "*   boundary of hypercube via LHS\n",
        "*   boundary of n-sphere randomly\n",
        "*   boundary of pie shaped domain randomly\n",
        "\n",
        "A generator always accepts the parameters `d` (dimension), \n",
        "some domain parameters (here `a, b` or `R`) and interior or boundary parameters (here `batch_size_inner`, `num_in_one_hyperplane` or `batch_size_boundary`).\n",
        "By following this standard, new generators can be easily added."
      ],
      "metadata": {
        "id": "lUzr0N8yoUX1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5G1dYMaHqCl"
      },
      "outputs": [],
      "source": [
        "# # # # # # # # # # # # # \n",
        "# interior generator(s) #\n",
        "# # # # # # # # # # # # # \n",
        "\n",
        "def generate_interior_randomly(d, a, b, batch_size_inner):\n",
        "  # uniformly generate random points inside the hypercube\n",
        "    # we unstack along axis 0 in order to convert tensor of shape (d, batch_size_inner) \n",
        "    # into d lists of tensors/values (the \"x1\"-values, the \"x2\"-values, ...)\n",
        "  X_inner = tf.unstack(tf.random.uniform([d, batch_size_inner], a, b), axis=0) \n",
        "\n",
        "  # calc d dimensional \"volume\" of interior\n",
        "  V_inner = abs(b-a)**d\n",
        "\n",
        "  return X_inner, V_inner\n",
        "\n",
        "def generate_interior_via_LHS(d, a, b, batch_size_inner):\n",
        "  # generate points using Latin Hypercube sampling\n",
        "  sample = qmc.LatinHypercube(d=d).random(n=batch_size_inner)\n",
        "  sample = qmc.scale(sample, d*[a], d*[b])\n",
        "    # we unstack along axis 1 in order to convert batch_size_inner points into d lists of values (the \"x1\"-values, the \"x2\"-values, ...)\n",
        "  X_inner = tf.unstack(tf.convert_to_tensor(sample, dtype='float32'), axis=1) \n",
        "\n",
        "  # calc d dimensional \"volume\" of interior\n",
        "  V_inner = abs(b-a)**d\n",
        "\n",
        "  return X_inner, V_inner\n",
        "\n",
        "def generate_interior_n_sphere_randomly(d, R, batch_size_inner):\n",
        "  # This algorithm is due to Marsaglia 1972 and Muller (1959)\n",
        "\n",
        "  # generate normal deviates and normalize them \n",
        "  X, _ = tf.linalg.normalize(tf.random.normal([d, batch_size_inner]), ord=2, axis=0)\n",
        "  # uniformly generate random radii and rescale them \n",
        "  r = R*tf.pow(tf.random.uniform([batch_size_inner]), 1/d)\n",
        "  # scale the normalized normal deviates by r\n",
        "  X_inner = tf.unstack(r*X, axis=0)\n",
        "\n",
        "  # calc d dimensional \"volume\" of interior\n",
        "  if d%2 == 0:\n",
        "    V_inner = math.pi*R**2\n",
        "    start_dim = 4\n",
        "  else: \n",
        "    V_inner = 2*R\n",
        "    start_dim = 3\n",
        "  # we calculate the volume iteratively (instead via the closed form formula) to avoid large factorials and powers\n",
        "  # this way we can for example calc the volume for d=200 R=1.0 or d=2000 R=10.0\n",
        "  for k in range(start_dim,d+1,2):\n",
        "      V_inner *= 2*math.pi*R**2/k\n",
        "\n",
        "  return X_inner, V_inner\n",
        "\n",
        "def generate_interior_pie_randomly(d, gamma, R, batch_size_inner):\n",
        "  # Note that parameter d is passed but intentionally not used\n",
        "\n",
        "  # uniformly generate random polar coordinates\n",
        "  phi = tf.random.uniform([batch_size_inner], 0.0, gamma)\n",
        "  r = tf.sqrt(tf.random.uniform([batch_size_inner], 0.0, 1.0))*R # sqrt is necessary to have the same density near 0 as near R\n",
        "\n",
        "  # convert polar coordinates into cartesian coordinates\n",
        "  X_inner = [r*tf.cos(phi), r*tf.sin(phi)]\n",
        "\n",
        "  # calc 2D \"volume\" of interior\n",
        "  V_inner = gamma/2*R**2\n",
        "\n",
        "  # return generated inner points\n",
        "  return X_inner, V_inner\n",
        "\n",
        "# # # # # # # # # # # # #\n",
        "# boundary generator(s) #\n",
        "# # # # # # # # # # # # #\n",
        "\n",
        "def generate_boundary_regularly(d, a, b, num_along_one_axis):\n",
        "  X_boundary = []\n",
        "\n",
        "  base = [ [a + (b-a)*i/(num_along_one_axis-1)] for i in range(num_along_one_axis)]\n",
        "  # this method recursively generates all points of the hypercube [a,b]^d\n",
        "  # generate(0, False) = [ [a, generate(1, True)], [a+(b-a)*1/n, generate(1, False)], ..., [b, generate(1, True)] ]\n",
        "  #     .\n",
        "  #     .\n",
        "  #     .\n",
        "  # generate(d-1, True) = base = [ [a], [a+(b-a)*1/n] [a+(b-a)*2/n] ..., [b] ] (where n is num_along_one_axis-1) \n",
        "  # the parameter include_all_below informs layers one recursion level below that ALL points need to be generated\n",
        "  # this happens when the set of points already includes a or b, which means that we are on the boundary\n",
        "  def generate(depth, include_all_below):\n",
        "    points = []\n",
        "    # with each recursive call one dimension is added to the points in the list points\n",
        "    # because we only need d dimensional points, we need to stop the recursion at depth == d-1\n",
        "    if depth == d-1:\n",
        "      # only including the boundary points at the last recusion layer means only including a and b\n",
        "      points += base if include_all_below else [[a],[b]]\n",
        "    else:\n",
        "      # generate all points below (those are always needed for [a, generate()] and [b, generate()])\n",
        "      all_points_below = generate(depth+1, True)\n",
        "      # only generate points below with include_all_below = False when needed\n",
        "      points_below = all_points_below if include_all_below else generate(depth+1, False)\n",
        "\n",
        "      # [a, generate(depth+1, True)]\n",
        "      points += [tf.concat([all_points_below, tf.fill([len(all_points_below),1], a)], axis=1)]\n",
        "      for i in range(1,num_along_one_axis-1): \n",
        "          # [a+(b-a)*i/n, generate(depth+1, include_all_below)]\n",
        "          points += [tf.concat([points_below, tf.fill([len(points_below),1], a+(b-a)*i/(num_along_one_axis-1))], axis=1)]\n",
        "      # [b, generate(depth+1, True)]\n",
        "      points += [tf.concat([all_points_below, tf.fill([len(all_points_below),1], b)], axis=1)]\n",
        "      # concat all seperate lists of points to one list\n",
        "      points = tf.concat(points, axis=0)\n",
        "    \n",
        "    return points\n",
        "  # convert list of points into d lists of values (the \"x1\"-values, \"x2\"-values, ...)\n",
        "  X_boundary = tf.unstack(generate(0, False), axis=1)\n",
        "\n",
        "  # calc d-1 dimensional \"volume\" of boundary\n",
        "  V_boundary = 2*d*abs(b-a)**(d-1)\n",
        "\n",
        "  return X_boundary, V_boundary\n",
        "\n",
        "def generate_boundary_randomly(d, a, b, num_in_one_hyperplane):\n",
        "  hyperplane_list = []\n",
        "  for i in range(d):\n",
        "    for s in (a, b):\n",
        "        # we unstack along axis 0 in order to convert tensor of shape (d-1, num_in_one_hyperplane) \n",
        "        # into d-1 lists of tensors/values (the \"x1\"-values, the \"x2\"-values, ...)\n",
        "      hyperplane = tf.unstack(tf.random.uniform([d-1, num_in_one_hyperplane], a, b), axis=0) \n",
        "      hyperplane.insert(i, tf.fill([num_in_one_hyperplane], s))\n",
        "\n",
        "      hyperplane_list += [hyperplane]\n",
        "  X_boundary = tf.unstack(tf.concat(hyperplane_list, axis=1), axis=0)\n",
        "\n",
        "  # calc d-1 dimensional \"volume\" of boundary\n",
        "  V_boundary = 2*d*abs(b-a)**(d-1)\n",
        "\n",
        "  return X_boundary, V_boundary\n",
        "\n",
        "def generate_boundary_via_LHS(d, a, b, num_in_one_hyperplane):\n",
        "  hyperplane_list = []\n",
        "  for i in range(d):\n",
        "    for s in (a, b):\n",
        "      # generate points using Latin Hypercube sampling\n",
        "      sample = qmc.LatinHypercube(d=d-1).random(n=num_in_one_hyperplane)\n",
        "      sample = qmc.scale(sample, (d-1)*[a], (d-1)*[b])\n",
        "      \n",
        "      # we unstack along axis 1 in order to convert num_in_one_hyperplane points into d-1 lists of values (the \"x1\"-values, the \"x2\"-values, ...)\n",
        "      hyperplane = tf.unstack(tf.convert_to_tensor(sample, dtype='float32'), axis=1)\n",
        "      hyperplane.insert(i, tf.fill([num_in_one_hyperplane], s))\n",
        "\n",
        "      hyperplane_list += [hyperplane]\n",
        "  X_boundary = tf.unstack(tf.concat(hyperplane_list, axis=1), axis=0)\n",
        "\n",
        "  # calc d-1 dimensional \"volume\" of boundary\n",
        "  V_boundary = 2*d*abs(b-a)**(d-1)\n",
        "\n",
        "  return X_boundary, V_boundary\n",
        "\n",
        "def generate_boundary_n_sphere_randomly(d, R, batch_size_boundary):\n",
        "  # This algorithm is due to Marsaglia 1972 and Muller (1959)\n",
        "\n",
        "  # generate normal deviates and normalize them \n",
        "  X, _ = tf.linalg.normalize(tf.random.normal([d, batch_size_boundary]), ord=2, axis=0)\n",
        "  X_boundary = tf.unstack(R*X, axis=0)\n",
        "\n",
        "  # calc d dimensional \"volume\" of boundary\n",
        "  if d%2 == 0:\n",
        "    V_boundary = 2*math.pi*R\n",
        "    start_dim = 4\n",
        "  else: \n",
        "    V_boundary = 2\n",
        "    start_dim = 3\n",
        "  # we calculate the volume iteratively (instead via the closed form formula) to avoid large factorials and powers\n",
        "  # this way we can for example calc the volume for d=200 R=1.0 or d=2000 R=10.0\n",
        "  for k in range(start_dim,d+1,2):\n",
        "      V_boundary *= 2*math.pi*R**2/(k-2)\n",
        "\n",
        "  return X_boundary, V_boundary\n",
        "\n",
        "def generate_boundary_pie_randomly(d, gamma, R, num_boundary_0, num_boundary_1, num_boundary_2):\n",
        "  # Note that parameter d is passed but intentionally not used\n",
        "\n",
        "  # generate boundary points for the line where phi=0\n",
        "  r = tf.random.uniform([num_boundary_0], 0, R)\n",
        "  X_boundary_0 = [r*math.cos(0), r*math.sin(0)]\n",
        "  \n",
        "  # generate boundary points for the arc where phi in [0, gamma]\n",
        "  phi = tf.random.uniform([num_boundary_1], 0, gamma)\n",
        "  X_boundary_1 = [R*tf.math.cos(phi), R*tf.math.sin(phi)]\n",
        "\n",
        "  # generate boundary points for the line where phi=gamma\n",
        "  r = tf.linspace(0.0, R, num_boundary_2)\n",
        "  X_boundary_2 = [r*math.cos(gamma), r*math.sin(gamma)]\n",
        "\n",
        "  # combine all generated boundary points\n",
        "    # after concatenating all boundary points we get a tensor of shape (d, num_boundary_0 + num_boundary_1 + num_boundary_2) \n",
        "    # but because we later need lists of tensors/values (the \"x\"-values, the \"y\"-values) we unstack the tensor along axis 0\n",
        "  X_boundary = tf.unstack(tf.concat([X_boundary_0, X_boundary_1, X_boundary_2], axis=1), axis=0)\n",
        "\n",
        "  # calc 1D \"volume\" of boundary\n",
        "  V_boundary = gamma*R\n",
        "\n",
        "  # return generated boundary points\n",
        "  return X_boundary, V_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3ntwVpOZ8Et"
      },
      "source": [
        "## Right-Hand Side and Exact Solution\n",
        "\n",
        "The following functions define the different right-hand sides and boundary conditions. As the boundary condition is always included in the exact solution and we are only interested in examples where we know the exact solution in order to calculate the approximation error, we decided to directly define the exact solution instead of the boundary condition. \n",
        "\n",
        "The parameters of the functions are the domain parameters (e.g. `a, b` or `R`) that have to match the domain parameters of the generator used later on during training and `X` containing the points where we want to evaluate the right-hand side or exact solution.\n",
        "\n",
        "The parameter `X` is an `n` dimensional array, \n",
        "where `X[i]` corresponds to a list of the i-th components of all points inside `X`, i.e. `X[i][j]` is the i-th component of the j-th point."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The right-hand sides and exact solutions defined here correspond to the following sections in the thesis:\n",
        "\n",
        "*   `sum_pow_x_2` $\\rightarrow$ **section 3.1** Finite Differences vs Autodiff in High Dimensions\n",
        "*   `sol_sum_abs_x_i` $\\rightarrow$ **section 3.3** The Solution Has no Weak $2^\\text{nd}$ Derivative\n",
        "*   `rhs_Q_Han_F_Lin_page_65` $\\rightarrow$ **section 3.4** The $2^\\text{nd}$ Derivative of the Solution Explodes"
      ],
      "metadata": {
        "id": "YclfCUpXdG9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixeOxlBfaCTb"
      },
      "outputs": [],
      "source": [
        "# DRM paper d=100 example\n",
        "def rhs_minus_2_d(a, b, X): # start rhs functions with rhs to make better use of auto complete\n",
        "  return tf.fill([len(X[0])], -2.0*len(X)) \n",
        "# the last function defined as rhs will be visualized \n",
        "rhs = rhs_minus_2_d \n",
        "\n",
        "def sol_sum_pow_x_2(a, b, X):\n",
        "  return tf.reduce_sum(tf.pow(X, 2), axis=0)\n",
        "# the last function defined as exact_sol will be visualized\n",
        "exact_sol = sol_sum_pow_x_2\n",
        "\n",
        "# parameters defining the domain and boundary\n",
        "  # this is only needed for visualization\n",
        "domain_params = [0.0, 1.0] # [a,b]\n",
        "generate_interior = generate_interior_randomly\n",
        "generate_boundary = generate_boundary_randomly\n",
        "interior_params, boundary_params, interior_check_params = [1_000], [100], [10_000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-entrant corner\n",
        "def rhs_zero(gamma, R, X):\n",
        "  return tf.zeros(len(X[0]))\n",
        "rhs = rhs_zero\n",
        "\n",
        "def sol_r_sin_phi(gamma, R, X):\n",
        "  r = tf.sqrt(tf.reduce_sum(tf.pow(X, 2), axis=0))\n",
        "  phi = tf.math.floormod(tf.atan2(X[1], X[0]), 2*math.pi) # mod 2*pi to change atan2 range of [-pi,pi] to [0, 2*pi]\n",
        "\n",
        "  return tf.pow(r, math.pi/gamma)*tf.sin(phi*math.pi/gamma)\n",
        "exact_sol = sol_r_sin_phi\n",
        "\n",
        "# parameters defining the domain and boundary\n",
        "  # this is only needed for visualization\n",
        "domain_params = [1.5*math.pi, 1.0] # [gamma, R]\n",
        "generate_interior = generate_interior_pie_randomly\n",
        "generate_boundary = generate_boundary_pie_randomly\n",
        "interior_params, boundary_params, interior_check_params = [1_000], [10, 100, 10], [10_000]"
      ],
      "metadata": {
        "id": "hRS8MPm64Arx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2aPBz-cgBya"
      },
      "outputs": [],
      "source": [
        "# no weak 2nd derivative\n",
        "def rhs_zero(a, b, X): \n",
        "  return tf.fill([len(X[0])], 0.0) \n",
        "rhs = rhs_zero \n",
        "\n",
        "def sol_sum_abs_x_i(a, b, X): \n",
        "  return tf.reduce_sum(tf.abs(X), axis=0)\n",
        "exact_sol = sol_sum_abs_x_i \n",
        "\n",
        "# parameters defining the domain and boundary\n",
        "  # this is only needed for visualization\n",
        "domain_params = [-1.0, 1.0] # [a,b]\n",
        "generate_interior = generate_interior_randomly\n",
        "generate_boundary = generate_boundary_randomly\n",
        "interior_params, boundary_params, interior_check_params = [1_000], [100], [10_000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW0e4Rxdd46G"
      },
      "outputs": [],
      "source": [
        "# f, g in C but u not in C2\n",
        "def rhs_Q_Han_F_Lin_page_65(R, X):\n",
        "  assert len(X) >= 2\n",
        "  d = len(X)\n",
        "  r = tf.sqrt(tf.reduce_sum(tf.pow(X, 2), axis=0))\n",
        "  m_ln_r = -tf.math.log(r)\n",
        "  return tf.where(r == 0, 0.0, (X[0]**2-X[1]**2)/(2*r**2)*((d+2)/tf.pow(m_ln_r, 0.5) + 1/(2*tf.pow(m_ln_r, 1.5))))\n",
        "rhs = rhs_Q_Han_F_Lin_page_65\n",
        "\n",
        "def sol_Q_Han_F_Lin_page_65(R, X):\n",
        "  assert len(X) >= 2\n",
        "  r = tf.sqrt(tf.reduce_sum(tf.pow(X, 2), axis=0))\n",
        "  return tf.where(r == 0, 0.0, (X[0]**2-X[1]**2)*tf.pow(-tf.math.log(r), 0.5))\n",
        "exact_sol = sol_Q_Han_F_Lin_page_65\n",
        "\n",
        "# parameters defining the domain and boundary\n",
        "  # this is only needed for visualization\n",
        "domain_params = [0.5] # [R]\n",
        "generate_interior = generate_interior_n_sphere_randomly\n",
        "generate_boundary = generate_boundary_n_sphere_randomly\n",
        "interior_params, boundary_params, interior_check_params = [1_000], [1_000], [10_000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are other possible right-hand sides and exact solutions not included in the thesis."
      ],
      "metadata": {
        "id": "Qg5asHqDdrtS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhdHvOyjEmOj"
      },
      "outputs": [],
      "source": [
        "# no weak 2nd derivative\n",
        "def rhs_zero(a, b, X): \n",
        "  return tf.fill([len(X[0])], 0.0) \n",
        "rhs = rhs_zero \n",
        "\n",
        "def sol_prod_abs_x_i(a, b, X): \n",
        "  return tf.reduce_prod(tf.abs(X), axis=0)\n",
        "exact_sol = sol_prod_abs_x_i \n",
        "\n",
        "# parameters defining the domain and boundary\n",
        "  # this is only needed for visualization\n",
        "domain_params = [-1.0, 1.0] # [a,b]\n",
        "generate_interior = generate_interior_randomly\n",
        "generate_boundary = generate_boundary_randomly\n",
        "interior_params, boundary_params, interior_check_params = [1_000], [100], [10_000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDbpa7-t0LJU"
      },
      "outputs": [],
      "source": [
        "# unbounded 2nd derivative\n",
        "def rhs_r_pow_minus_d_half(R, X):\n",
        "  d = len(X)\n",
        "  r = tf.sqrt(tf.reduce_sum(tf.pow(X, 2), axis=0))\n",
        "  return (d/2)*(d/2-2)*tf.minimum(tf.pow(r, -d/2), 1000) # min(..., 1000) avoids infinities\n",
        "rhs = rhs_r_pow_minus_d_half\n",
        "\n",
        "def sol_r_pow_2_minus_d_half(R, X): \n",
        "  d = len(X)\n",
        "  r = tf.sqrt(tf.reduce_sum(tf.pow(X, 2), axis=0))\n",
        "  return tf.minimum(tf.pow(r, 2-d/2), 1000) # min(..., 1000) avoids infinities\n",
        "exact_sol = sol_r_pow_2_minus_d_half\n",
        "\n",
        "# parameters defining the domain and boundary\n",
        "  # this is only needed for visualization\n",
        "domain_params = [1.0] # [R]\n",
        "generate_interior = generate_interior_n_sphere_randomly\n",
        "generate_boundary = generate_boundary_n_sphere_randomly\n",
        "interior_params, boundary_params, interior_check_params = [1_000], [1_000], [10_000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuXG5Cz3gY81"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbSg5ERk8xkB"
      },
      "outputs": [],
      "source": [
        "d = 2 # <--- you can adjust the input dimension by changing this value\n",
        "      #      the visualization of re-entrant corners only works for d=2\n",
        "# the meaning of the content of interior_params, boundary_params and interior_check_params depends on the generator used\n",
        "\n",
        "X_inner, _ = generate_interior(d, *domain_params, *interior_params)\n",
        "f = rhs(*domain_params, X_inner)\n",
        "X_check, _ = generate_interior(d, *domain_params, *interior_check_params)\n",
        "u_exact = exact_sol(*domain_params, X_check)\n",
        "X_boundary, _ = generate_boundary(d, *domain_params, *boundary_params)\n",
        "g = exact_sol(*domain_params, X_boundary)\n",
        "\n",
        "if d == 2:  \n",
        "  # create a figure with three subplots (left, middle, right)\n",
        "  fig = make_subplots(rows=1, cols=3, \n",
        "                      subplot_titles=(\"points: inner, height: f\", \"points: check, height: u_exact\", \"points: boundary, height: g\"), \n",
        "                      specs=[[{'type': 'scene'}, {'type': 'scene'}, {'type': 'scene'}]])\n",
        "  fig.update_layout(height=500, width=1200, showlegend=False)\n",
        "  # add subplot of f plotted against the points generated inside the domain to the left of the figure\n",
        "  fig.add_traces(\n",
        "      data=[go.Scatter3d(x=X_inner[0], y=X_inner[1], z=f, mode='markers'),],\n",
        "      rows=1, cols=1\n",
        "  )\n",
        "  # add subplot of u_exact plotted against the points generated to check approximations to the middle of the figure\n",
        "  fig.add_traces(\n",
        "      data=[go.Scatter3d(x=X_check[0], y=X_check[1], z=u_exact, mode='markers'),],\n",
        "      rows=1, cols=2\n",
        "  )\n",
        "  # add subplot of g plotted against the points generated at the boundary to the right of the figure\n",
        "  fig.add_traces(\n",
        "      data=[go.Scatter3d(x=X_boundary[0], y=X_boundary[1], z=g, mode='markers'),],\n",
        "      rows=1, cols=3\n",
        "  )\n",
        "  fig.show()\n",
        "elif d > 2:\n",
        "  # create a figure with three subplots (left, middle, right)\n",
        "  fig = make_subplots(rows=1, cols=3, \n",
        "                      subplot_titles=(\"X_inner, color: f\", \"X_check, color: u_exact\", \"X_boundary, color: g\"), \n",
        "                      specs=[[{'type': 'scene'}, {'type': 'scene'}, {'type': 'scene'}]])\n",
        "  fig.update_layout(height=500, width=1200, showlegend=False)\n",
        "  # add subplot of inner points generated inside the domain colored according to f to the left of the figure\n",
        "  fig.add_traces(\n",
        "      data=[go.Scatter3d(x=X_inner[0], y=X_inner[1], z=X_inner[2], \n",
        "                        mode='markers', marker= dict(size=5,\n",
        "                                                      color=f, # set color to an array/list of desired values\n",
        "                                                      colorscale='Viridis',   # choose a colorscale\n",
        "                                                      opacity=1), \n",
        "                        ),\n",
        "            ],\n",
        "      rows=1, cols=1,\n",
        "  )\n",
        "  # add subplot of points generated to check approximations colored according to u_exact to the middle of the figure\n",
        "  fig.add_traces(\n",
        "      data=[go.Scatter3d(x=X_check[0], y=X_check[1], z=X_check[2], \n",
        "                        mode='markers', marker= dict(size=5,\n",
        "                                                      color=u_exact, # set color to an array/list of desired values\n",
        "                                                      colorscale='Viridis',   # choose a colorscale\n",
        "                                                      opacity=1)\n",
        "                        ),\n",
        "            ],\n",
        "      rows=1, cols=2\n",
        "  )\n",
        "  # add subplot of points genearted at the boundary colored according to g to the right of the figure\n",
        "  fig.add_traces(\n",
        "      data=[go.Scatter3d(x=X_boundary[0], y=X_boundary[1], z=X_boundary[2], \n",
        "                        mode='markers', marker= dict(size=5,\n",
        "                                                      color=g, # set color to an array/list of desired values\n",
        "                                                      colorscale='Viridis', \n",
        "                                                      opacity=1)\n",
        "                        ),\n",
        "            ],\n",
        "      rows=1, cols=3\n",
        "  )\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvpa-x1k6t8"
      },
      "source": [
        "# PINN & DRM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions in the following cell define how to perform a train step with PINNs and with the DRM. The details of why this results in the neural network approximating the exact solution are explained in **section 2.1 and 2.2** of the thesis."
      ],
      "metadata": {
        "id": "IWu3TWDchWD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZZ08SOuk5Pe"
      },
      "outputs": [],
      "source": [
        "def train_step_PINN(X_inner, V_inner, f, X_boundary, V_boundary, g, model, beta, opt):\n",
        "  # X_inner and X_boundary should contain data of the same dimension\n",
        "  assert len(X_inner) == len(X_boundary) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_inner[i]) == len(X_inner[i+1]) for i in range(len(X_inner)-1)]) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_boundary[i]) == len(X_boundary[i+1]) for i in range(len(X_boundary)-1)]) \n",
        "  # f should contain as many values as we have passed inputs to rhs\n",
        "  assert len(X_inner[0]) == len(f) \n",
        "  # g should contain as many values as we have passed inputs to exact_sol\n",
        "  assert len(X_boundary[0]) == len(g) \n",
        "  \n",
        "  d = len(X_inner)\n",
        "  batch_size_inner = len(X_inner[0]) \n",
        "  batch_size_boundary = len(X_boundary[0])\n",
        "\n",
        "  with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape(persistent=True) as tape2:\n",
        "      tape2.watch(X_inner)\n",
        "      with tf.GradientTape() as tape1:\n",
        "        tape1.watch(X_inner)\n",
        "        \n",
        "        batch = tf.stack(X_inner, axis=1)\n",
        "        u = model(batch)\n",
        "        u = tf.reshape(u, [batch_size_inner])\n",
        "      udx = tape1.gradient(u, X_inner)\n",
        "    udxx = [tape2.gradient(udx[i], X_inner[i]) for i in range(d)]\n",
        "    tape2.stop_recording()\n",
        "\n",
        "    batch_boundary = tf.stack(X_boundary, axis=1)\n",
        "    u_boundary = model(batch_boundary)\n",
        "    u_boundary = tf.reshape(u_boundary, [batch_size_boundary])\n",
        "\n",
        "    laplacian = tf.reduce_sum(udxx, axis=0) \n",
        "    loss_inner = V_inner/batch_size_inner * tf.reduce_sum((laplacian+f)**2, axis=0) # -Δu = f \n",
        "    loss_boundary = V_boundary/batch_size_boundary * tf.reduce_sum((u_boundary - g)**2, axis=0)\n",
        "      \n",
        "      # Alternative balanced loss functions (used in section 3.4)\n",
        "    # loss_inner = (V_inner/V_boundary) * (1/batch_size_inner) * tf.reduce_sum((laplacian+f)**2, axis=0) # -Δu = f \n",
        "    # loss_boundary = (1/batch_size_boundary) * tf.reduce_sum((u_boundary - g)**2, axis=0)\n",
        "    loss = loss_inner + beta*loss_boundary\n",
        "  grad = tape3.gradient(loss, model.trainable_weights)\n",
        "  opt.apply_gradients(zip(grad, model.trainable_weights))\n",
        "\n",
        "  return loss, loss_inner, loss_boundary\n",
        "\n",
        "def train_step_DRM(X_inner, V_inner, f, X_boundary, V_boundary, g, model, beta, opt):\n",
        "  # X_inner and X_boundary should contain data of the same dimension\n",
        "  assert len(X_inner) == len(X_boundary) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_inner[i]) == len(X_inner[i+1]) for i in range(len(X_inner)-1)]) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_boundary[i]) == len(X_boundary[i+1]) for i in range(len(X_boundary)-1)]) \n",
        "  # f should contain as many values as we have passed inputs to rhs\n",
        "  assert len(X_inner[0]) == len(f) \n",
        "  # g should contain as many values as we have passed inputs to exact_sol\n",
        "  assert len(X_boundary[0]) == len(g) \n",
        "  \n",
        "  batch_size_inner = len(X_inner[0]) \n",
        "  batch_size_boundary = len(X_boundary[0])\n",
        "\n",
        "  with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "      tape1.watch(X_inner)\n",
        "      \n",
        "      batch = tf.stack(X_inner, axis=1)\n",
        "      u = model(batch)\n",
        "      u = tf.reshape(u, [batch_size_inner])\n",
        "    udx = tape1.gradient(u, X_inner)\n",
        "\n",
        "    batch_boundary = tf.stack(X_boundary, axis=1)\n",
        "    u_boundary = model(batch_boundary)\n",
        "    u_boundary = tf.reshape(u_boundary, [batch_size_boundary])\n",
        "    \n",
        "                                         #  tf.reduce_sum(tf.pow(udx, 2), axis=0) corresponds to |∇u|²\n",
        "    loss_inner = V_inner/batch_size_inner * tf.reduce_sum(0.5*tf.reduce_sum(tf.pow(udx, 2), axis=0)-f*u, axis=0) # 1/2*|∇u|²-f*u\n",
        "    loss_boundary = V_boundary/batch_size_boundary * tf.reduce_sum((u_boundary - g)**2, axis=0)\n",
        "      \n",
        "      # Alternative balanced loss functions (used in section 3.4)\n",
        "    # loss_inner = (V_inner/V_boundary)*(1/batch_size_inner) * tf.reduce_sum(0.5*tf.reduce_sum(tf.pow(udx, 2), axis=0)-f*u, axis=0) # 1/2*|∇u|²-f*u\n",
        "    # loss_boundary = (1/batch_size_boundary) * tf.reduce_sum((u_boundary - g)**2, axis=0)\n",
        "    loss = loss_inner + beta*loss_boundary\n",
        "  grad = tape2.gradient(loss, model.trainable_weights)\n",
        "  opt.apply_gradients(zip(grad, model.trainable_weights))\n",
        "\n",
        "  return loss, loss_inner, loss_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A1nXQf0oaDt"
      },
      "source": [
        "## Alt: Train Step Using Finite Differences \n",
        "\n",
        "You can execute the following cell in order to train the neural networks with finite differences instead of autodiff. This was for example used for the runtime tests in **section 3.1**.\n",
        "\n",
        "In order to revert back to using autodiff simply overwrite the functions `train_step_PINN` and `train_step_DRM` by running the last cell again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYTpuxj8obBb"
      },
      "outputs": [],
      "source": [
        "h = 1e-3 # step size of finite differences\n",
        "\n",
        "def train_step_PINN(X_inner, V_inner, f, X_boundary, V_boundary, g, model, beta, opt):\n",
        "  # X_inner and X_boundary should contain data of the same dimension\n",
        "  assert len(X_inner) == len(X_boundary) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_inner[i]) == len(X_inner[i+1]) for i in range(len(X_inner)-1)]) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_boundary[i]) == len(X_boundary[i+1]) for i in range(len(X_boundary)-1)]) \n",
        "  # f should contain as many values as we have passed it inputs\n",
        "  assert len(X_inner[0]) == len(f) \n",
        "  # g should contain as many values as we have passed it inputs\n",
        "  assert len(X_boundary[0]) == len(g) \n",
        "  \n",
        "  d = len(X_inner)\n",
        "  batch_size_inner = len(X_inner[0]) \n",
        "  batch_size_boundary = len(X_boundary[0])\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    batch = tf.stack(X_inner, axis=1)\n",
        "\n",
        "    u = model(batch)\n",
        "    u = tf.reshape(u, [batch_size_inner])\n",
        "\n",
        "    laplacian = 0\n",
        "    for i in range(d):\n",
        "      u_p = model(batch+tf.one_hot([i], d, h)) # u(x + e_i*h) p: plus\n",
        "      u_p = tf.reshape(u_p, [batch_size_inner])\n",
        "\n",
        "      u_m = model(batch-tf.one_hot([i], d, h)) # u(x - e_i*h) m: minus\n",
        "      u_m = tf.reshape(u_m, [batch_size_inner])\n",
        "\n",
        "      laplacian += (u_p -2*u +u_m)/h**2\n",
        "\n",
        "    batch_boundary = tf.stack(X_boundary, axis=1)\n",
        "    u_boundary = model(batch_boundary)\n",
        "    u_boundary = tf.reshape(u_boundary, [batch_size_boundary])\n",
        "\n",
        "    loss_inner = V_inner/batch_size_inner * tf.reduce_sum((laplacian+f)**2, axis=0) # -Δu = f \n",
        "    loss_boundary = V_boundary/batch_size_boundary * tf.reduce_sum((u_boundary - g)**2, axis=0)\n",
        "    loss = loss_inner + beta*loss_boundary\n",
        "  grad = tape.gradient(loss, model.trainable_weights)\n",
        "  opt.apply_gradients(zip(grad, model.trainable_weights))\n",
        "\n",
        "  return loss, loss_inner, loss_boundary\n",
        "\n",
        "def train_step_DRM(X_inner, V_inner, f, X_boundary, V_boundary, g, model, beta, opt):\n",
        "  # X_inner and X_boundary should contain data of the same dimension\n",
        "  assert len(X_inner) == len(X_boundary) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_inner[i]) == len(X_inner[i+1]) for i in range(len(X_inner)-1)]) \n",
        "  # Every point in X_inner should have a component in all d coordinates\n",
        "  assert all([len(X_boundary[i]) == len(X_boundary[i+1]) for i in range(len(X_boundary)-1)]) \n",
        "  # f should contain as many values as we have passed it inputs\n",
        "  assert len(X_inner[0]) == len(f) \n",
        "  # g should contain as many values as we have passed it inputs\n",
        "  assert len(X_boundary[0]) == len(g) \n",
        "  \n",
        "  d = len(X_inner)\n",
        "  batch_size_inner = len(X_inner[0]) \n",
        "  batch_size_boundary = len(X_boundary[0])\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    batch = tf.stack(X_inner, axis=1)\n",
        "\n",
        "    u = model(batch)\n",
        "    u = tf.reshape(u, [batch_size_inner])\n",
        "\n",
        "    nabla_u_2 = 0 # will become ∇u squared component wise\n",
        "    for i in range(d):\n",
        "      u_p = model(batch+tf.one_hot([i], d, h)) # u(x + e_i*h) p: plus\n",
        "      u_p = tf.reshape(u_p, [batch_size_inner])\n",
        "\n",
        "      nabla_u_2 += ((u_p -u)/h)**2 \n",
        "\n",
        "    batch_boundary = tf.stack(X_boundary, axis=1)\n",
        "    u_boundary = model(batch_boundary)\n",
        "    u_boundary = tf.reshape(u_boundary, [batch_size_boundary])\n",
        "    \n",
        "    loss_inner = V_inner/batch_size_inner * tf.reduce_sum(0.5*nabla_u_2-f*u, axis=0) # 1/2*|∇u|²-f*u\n",
        "    loss_boundary = V_boundary/batch_size_boundary * tf.reduce_sum((u_boundary - g)**2, axis=0)\n",
        "    loss = loss_inner + beta*loss_boundary\n",
        "  grad = tape.gradient(loss, model.trainable_weights)\n",
        "  opt.apply_gradients(zip(grad, model.trainable_weights))\n",
        "\n",
        "  return loss, loss_inner, loss_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uRuxvoHHsHq"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics"
      ],
      "metadata": {
        "id": "-tftZ51IjgxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the following cell, will create the functions used to record the relative $L^2$-, $H^1$-, $L^\\infty$- and $C^1$-errors introduced in **section 2.6** of the thesis.\n",
        "\n",
        "You can define custom metrics by following the structure of the code below."
      ],
      "metadata": {
        "id": "4CJlPqdfjyf7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfjtgbEi_1Un"
      },
      "outputs": [],
      "source": [
        "# define additional metrics that can be recorded during training\n",
        "\n",
        "# a metric function for the relative L2 error to wrap in MeanMetricWrapper \n",
        "def fn_relative_L2(y_true, y_pred):\n",
        "  # y_true and y_pred are lists of the form [u, udx[0], ..., udx[d]] \n",
        "\n",
        "  # note: because we use the RELATIVE L2 norm the term \"Volume/num\" in the denominator \n",
        "  #       is cancelled by the same term in the enumerator\n",
        "  return tf.norm(y_true[0]-y_pred[0], 2)/tf.norm(y_true[0], 2)\n",
        "# create the relative L2 error metric from MeanMetricWrapper\n",
        "  # Q: why do we need a mean to calc the relative L2 norm?\n",
        "  # A: we don't. Metrics in tf can record multiple errors before calculating their result\n",
        "  #    In the case of a MeanMetricWrapper, the result will be the mean of the recorded errors\n",
        "  #    the metrics used in the training loop will however always record only ONE error \n",
        "metric_relative_L2 = keras.metrics.MeanMetricWrapper(fn_relative_L2, name='relative_L2_error')\n",
        "\n",
        "\n",
        "# a metric function for the relative H1 error to wrap in MeanMetricWrapper \n",
        "def fn_relative_H1(y_true, y_pred):\n",
        "  # y_true and y_pred are lists of the form [u, udx[0], ..., udx[d-1]] \n",
        "  if len(y_true) <= 1: return np.nan\n",
        "  y_difference, y = 0, 0\n",
        "  for i in range(len(y_true)):\n",
        "    y_difference += tf.norm(y_true[i]-y_pred[i], 2)\n",
        "    y += tf.norm(y_true[i], 2)\n",
        "  # note: because we use the RELATIVE H1 norm the term \"Volume/num\" in the denominator \n",
        "  #       is cancelled by the same term in the enumerator and an increased number of \n",
        "  #       partial derivatives in the enumarator is balanced by the same increase in the denominator\n",
        "  return y_difference/y\n",
        "# create the relative H1 error metric from MeanMetricWrapper\n",
        "metric_relative_H1 = keras.metrics.MeanMetricWrapper(fn_relative_H1, name='relative_H1_error')\n",
        "\n",
        "\n",
        "# a metric function for the relative L_oo error to wrap in MeanMetricWrapper \n",
        "def fn_relative_L_oo(y_true, y_pred):\n",
        "  # y_true and y_pred are lists of the form [u, udx[0], ..., udx[d-1]] \n",
        "  return tf.norm(y_true[0]-y_pred[0], np.inf)/tf.norm(y_true[0], np.inf)\n",
        "# create the relative L_oo error metric from MeanMetricWrapper\n",
        "metric_relative_L_oo = keras.metrics.MeanMetricWrapper(fn_relative_L_oo, name='relative_L_oo_error')\n",
        "\n",
        "# a metric function for the relative C1 error to wrap in MeanMetricWrapper \n",
        "def fn_relative_C1(y_true, y_pred):\n",
        "  # y_true and y_pred are lists of the form [u, udx[0], ..., udx[d-1]] \n",
        "  if len(y_true) <= 1: return np.nan\n",
        "  y_difference, y = 0, 0\n",
        "  for i in range(len(y_true)):\n",
        "    y_difference += tf.norm(y_true[i]-y_pred[i], np.inf)\n",
        "    y += tf.norm(y_true[i], np.inf)\n",
        "  return y_difference/y\n",
        "# create the relative c1 error metric from MeanMetricWrapper\n",
        "metric_relative_C1 = keras.metrics.MeanMetricWrapper(fn_relative_C1, name='relative_C1_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The \"Test API\""
      ],
      "metadata": {
        "id": "YdxYGIlqjj45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to allow the simple creation of a large number of different tests we wrote a small \"Test API\".\n",
        "\n",
        "> A test can be easily constructed by creating an instance of the Test class and passing all parameters we want to change. The unspecified parameters are asigned default values, which are defined in the `__init__` method.\n",
        "\n",
        "We typically wanted to create multiple tests at once, \n",
        "where all tests had a range of parameters in common and only a few parameters changed from test to test. For this we wrote the function `create_tests(*tests, group='', **params)`, which takes a list of Tests and parameters as its input and inserts the list of parameters into every Test. \n",
        "\n",
        "A Test created in this way is also assigned a *group* defined by the parameter `group` that is passed to `create_tests`. A Test can also have a name that is displayed during training and is used for logging. \n",
        "\n",
        "> Group names and test names can be generated automatically by the parameters of the group/test. For this simply write an underscore after the name of all the parameters you want to include in the name. For example: \n",
        "```\n",
        "create_tests(Test(learning_rate_ = 10, beta = 0), epochs_ = 2, logging_interval = 1) \n",
        "```\n",
        "will have the name 'learning_rate=10' and be in the group 'epochs=2'."
      ],
      "metadata": {
        "id": "q1CiKjIilJyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA8SRGp8aPTK"
      },
      "outputs": [],
      "source": [
        "# this is mearly a parser in order to convert attributes of a class into an easier to read format\n",
        "# this is for example used in order to create the names of test-groups or test-names\n",
        "# but also for the description of a test\n",
        "def to_readable_str(val):\n",
        "  string = ''\n",
        "  place_holder = '{}'\n",
        "  iterator = [val]\n",
        "  try: \n",
        "    if type(val) == str:\n",
        "      place_holder = '\"{}\"'\n",
        "    else:\n",
        "      iterator = iter(val)\n",
        "      place_holder = '[{}]'\n",
        "  finally:\n",
        "    for el in iterator:\n",
        "      string += ', ' if string != '' else ''\n",
        "      if 'name' in dir(el): \n",
        "        string += el.name\n",
        "      elif '__name__' in dir(el):\n",
        "        string += el.__name__\n",
        "      else:\n",
        "        string += str(el)\n",
        "    return place_holder.format(string)\n",
        "    \n",
        "# the Test class with default test values\n",
        "class Test:\n",
        "  # change parameters here to change them for every test at once\n",
        "  def __init__(self, *, name = '',\n",
        "               logging_interval = 100, printing_interval = 1000, # both in epochs\n",
        "               input_dim = 3, num_blocks = 4, nodes_per_layer = 10, \n",
        "               activation = keras.activations.swish,  \n",
        "                # you can use any function defined in keras.activations\n",
        "                # or define your own using keras.layers.Activation(lambda x: x, name='example activation')\n",
        "               generate_interior = generate_interior_via_LHS, generate_interior_check = generate_interior_randomly, \n",
        "               generate_boundary = generate_boundary_via_LHS, generate_boundary_check = generate_boundary_randomly, \n",
        "               rhs = None, exact_sol = None, # TODO: do you want to specify not None default values for this? \n",
        "               domain_params = [-1.0, 1.0], # [a: interval start, b: interval end] the hypercube is [a,b]^input_dim\n",
        "               interior_params = [1_000], interior_check_params = [10_000], # [batch_size_inner: size of batches containing points in the interior of the domain]\n",
        "               boundary_params = [100], boundary_check_params = [1000], # num_boundary_along_one_axis: number of boundary points along one axis\n",
        "               epochs = 10_000, beta = 100, learning_rate = 0.001, \n",
        "               optimizer = keras.optimizers.Adam, metrics = [metric_relative_L2, metric_relative_H1, metric_relative_L_oo], **kwargs):\n",
        "    self.name = name # tests have names in order to more easily identify them (e.g. test names are part of the file names of logs)\n",
        "    self.group = '' # tests belong to groups in order to better organize them (e.g. tests in one group will create logs in a seperate folder)\n",
        "\n",
        "    self.logging_interval = logging_interval # interval in epochs between logging \n",
        "    self.printing_interval = printing_interval # inteval in epochs between printing\n",
        "\n",
        "    self.input_dim = input_dim # input dimension of neural network\n",
        "    self.num_blocks = num_blocks # number of residual blocks\n",
        "    self.nodes_per_layer = nodes_per_layer # the layer size used for all hidden layers \n",
        "    self.activation = activation # the activation function used for all hidden layers\n",
        "\n",
        "    self.generate_interior = generate_interior # TODO: explain\n",
        "    self.generate_interior_check = generate_interior_check # TODO: explain\n",
        "    self.generate_boundary = generate_boundary # TODO: explain\n",
        "    self.generate_boundary_check = generate_boundary_check # TODO: explain\n",
        "\n",
        "    self.rhs = rhs # TODO: explain\n",
        "    self.exact_sol = exact_sol # TODO: explain\n",
        "\n",
        "    self.domain_params = domain_params # a tuple defining the parameters regarding the domain needed by the generators \n",
        "    self.interior_params = interior_params # a tuple defining the parameters (appart from domain params) generate_interior expects\n",
        "    self.interior_check_params = interior_check_params # a tuple defining the parameters (appart from domain params) generate_interior_check expects\n",
        "    self.boundary_params = boundary_params # a tuple defining the parameters (appart from domain params) generate_boundary expects\n",
        "    self.boundary_check_params = boundary_check_params # a tuple defining the parameters (appart from domain params) generate_boundary_check expects\n",
        "    # those parameters are passed like this: generate_abc(*domain_params, *abc_params)\n",
        "\n",
        "    self.epochs = epochs # (in this case) total number of training steps\n",
        "    self.beta = beta # weight of boundary condition penalty term\n",
        "    self.learning_rate = learning_rate # learning rate passed to the optimization algorithm used\n",
        "    self.optimizer = optimizer # optimization algorithm used to update weights and biases\n",
        "    self.metrics = metrics # list of metrics that should be logged and printed during each test\n",
        "\n",
        "    self.kwargs = kwargs\n",
        "\n",
        "  def description(self):\n",
        "    # this method returns a complete description of the test \n",
        "    text = ''\n",
        "    for attr in dir(self):\n",
        "      if not attr.startswith('__') and attr not in ['kwargs', 'description']:\n",
        "        text += ', ' if text != '' else ''\n",
        "        text += f\"{attr}={to_readable_str(getattr(self, attr))}\"\n",
        "    return text\n",
        "\n",
        "# use this method to create multiple tests at once that share some parameters\n",
        "  # create_tests automatically creates a group if none was passed (i.e. group='') based on the parameters passed to params that end with _\n",
        "  # create_tests automatically creates names for tests without names (i.e. name = '') based on the parameters in kwargs of the test that end with _\n",
        "  # e.g.\n",
        "  # the test in the list created by create_tests(Test(learning_rate_ = 10, beta = 0), epochs_ = 2, logging_interval = 1) \n",
        "  # will have the name 'learning_rate=10' and be in the group 'epochs=2' and \n",
        "  # (beta and logging_interval are not included because they don't end in an underscore \"_\")\n",
        "def create_tests(*tests, group='', **params):\n",
        "  for t in tests:\n",
        "    t.group = group\n",
        "\n",
        "    # go over kwargs\n",
        "    for key in t.kwargs:\n",
        "      attr = key[:-1]\n",
        "      if key.endswith('_') and hasattr(t, attr):\n",
        "        val = t.kwargs[key]\n",
        "        # group params always override test params\n",
        "        if key in params: val = params[key]\n",
        "        if attr in params: val = params[attr]\n",
        "        setattr(t, attr, val)\n",
        "        t.name += ', ' if t.name != '' else ''\n",
        "        t.name += f\"{attr}={to_readable_str(val)}\"\n",
        "    # go over params\n",
        "    for key in params:\n",
        "      attr = key[:-1] if key.endswith('_') else key\n",
        "      if hasattr(t, attr):\n",
        "        setattr(t, attr, params[key])\n",
        "        # if no group, set group\n",
        "        if key.endswith('_'):\n",
        "          t.group += ', ' if t.group != '' else ''\n",
        "          t.group += f\"{attr}={to_readable_str(params[key])}\"\n",
        "  return tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrMbFAh2gh_e"
      },
      "source": [
        "## Define All the Tests to Be Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2peiF9H5oAdF"
      },
      "outputs": [],
      "source": [
        "# all tests in the tests variable will be executed in the training loop\n",
        "tests = []\n",
        "\n",
        "# # # # # # # # # # # # # # #\n",
        "# define all the tests here #\n",
        "# # # # # # # # # # # # # # #\n",
        "\n",
        "# for example\n",
        "activation_relu_x3 = layers.Activation(lambda x: tf.pow(tf.maximum(0.0,(x/5)), 3), name='relu_x_div_5_pow_3')\n",
        "tests += create_tests(Test(domain_params_=[1.0, 0.5*math.pi]), Test(domain_params_=[1.0, 1.0*math.pi]),\n",
        "                      Test(domain_params_=[1.0, 1.5*math.pi]), Test(domain_params_=[1.0, 2.0*math.pi]),\n",
        "                      input_dim=2, epochs=5_000, activation_=activation_relu_x3, \n",
        "                      interior_params=[1_000], interior_check_params=[10_000], \n",
        "                      generate_interior=generate_interior_pie_randomly, generate_interior_check=generate_interior_pie_randomly,\n",
        "                      boundary_params=[10, 100, 10], boundary_check_params=[100, 1000, 100], \n",
        "                      generate_boundary=generate_boundary_pie_randomly, generate_boundary_check=generate_boundary_pie_randomly, \n",
        "                      rhs=rhs_zero, exact_sol=sol_r_sin_phi)\n",
        "\n",
        "# generate a summary of the all defined tests \n",
        "last_group = None\n",
        "for index, test in enumerate(tests, 1):\n",
        "  if last_group != test.group:\n",
        "    print(80*'-')\n",
        "    print(f\"Group: {test.group}\")\n",
        "    last_group = test.group\n",
        "  print(f\"  Test #{index}:\\t{test.name}\")\n",
        "print(80*'=')\n",
        "print(f\"Total number of tests: {len(tests)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSaF3SMhEmzg"
      },
      "source": [
        "# Training loop\n",
        "\n",
        "In the beginning of **section 2** of the thesis, we sketched out a training loop in pseudocode. In this section, the real counter part can be found. \n",
        "\n",
        "Executing the cell containing the training loop will commence\n",
        "training using PINNs and the DRM for all previously defined tests. \n",
        "\n",
        "After every `printing_interval` epochs, the current losses and all values of all performance metrics will be displayed.\n",
        "After every `logging_interval` epochs they will be logged.\n",
        "(`printing_interval` and `logging_interval` are parameters that can be passed to a Test)\n",
        "\n",
        "The logging structure is the following:\n",
        "```\n",
        "{log_base_route}/group#1/name#1\n",
        "```\n",
        "If the group already exists, the number behind group will increase to the next largest number that is not yet used. The same holds for the test name.\n",
        "The `log_base_rout` is either `logs` if the logs are not saved permanently in google drive or `content/drive/MyDrive/PINN_vs_DRM_tests/logs` otherwise.\n",
        "Both paths are set automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XqXLPG6zln-"
      },
      "outputs": [],
      "source": [
        "# this function calculates the values of the exact solution and \n",
        "# PINNs' and DRM's approximation of it, as well as all corresponding derivatives \n",
        "# at a given set of points (X_check).\n",
        "# those can then be used to calculate (relative) errors\n",
        "@tf.function \n",
        "def calc_u_check(X_check, exact_sol, model_PINN, model_DRM):\n",
        "  batch_size_check = len(X_check[0])\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch(X_check)\n",
        "    \n",
        "    u_exact = exact_sol(X_check)\n",
        "    u_exact = tf.reshape(u_exact, [batch_size_check])\n",
        "\n",
        "    batch = tf.stack(X_check, axis=1)\n",
        "    u_PINN = model_PINN(batch)\n",
        "    u_PINN = tf.reshape(u_PINN, [batch_size_check])\n",
        "    u_DRM = model_DRM(batch)\n",
        "    u_DRM = tf.reshape(u_DRM, [batch_size_check])\n",
        "  udx_exact = tape.gradient(u_exact, X_check)\n",
        "\n",
        "  udx_PINN = tape.gradient(u_PINN, X_check)\n",
        "  udx_DRM = tape.gradient(u_DRM, X_check)\n",
        "  tape.stop_recording()\n",
        "  \n",
        "  return u_exact, udx_exact, u_PINN, udx_PINN, u_DRM, udx_DRM\n",
        "\n",
        "# this function calculates the same variables as the one above but in this case for boundary points\n",
        "# we therefore do not need to calculate the derivatives\n",
        "def calc_u_boundary_check(X_boundary_check, exact_sol, model_PINN, model_DRM):\n",
        "  u_boundary_exact = exact_sol(X_boundary_check)\n",
        "  batch = tf.stack(X_boundary_check, axis=1)\n",
        "  u_boundary_PINN = model_PINN(batch)\n",
        "  u_boundary_DRM = model_DRM(batch)\n",
        "  return u_boundary_exact, u_boundary_PINN, u_boundary_DRM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ **Attention:** Running this cell will start the training loop!,\n",
        "which will immediately create the first logging folder and may take a while till completion. \n",
        "\n",
        "A running cell can be stopped by clicking on the corresponding button at the top left of the cell."
      ],
      "metadata": {
        "id": "7usUxR4ct4mY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_XJz3xOhiQM"
      },
      "outputs": [],
      "source": [
        "Path(log_base_route).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "last_group = None\n",
        "for index, test in enumerate(tests, 1):\n",
        "  print(80*\"#\")\n",
        "  print(f\"working on {index}/{len(tests)} {test.group}/{test.name} \")\n",
        "  print(80*\"#\")\n",
        "\n",
        "  # # # # # # # # # # # # # # # #\n",
        "  # set up logging and printing #\n",
        "  # # # # # # # # # # # # # # # #\n",
        "  \n",
        "  # create directory for logs of the current test\n",
        "  #   add test group and name to route to structure tests\n",
        "  #     if group name already exists append #num appropriately\n",
        "  #     if test name already exists append #num appropriately\n",
        "  if last_group != test.group:\n",
        "    test_num_dict = defaultdict(lambda:0)\n",
        "    group_num = 0\n",
        "    for group_name in listdir(log_base_route):\n",
        "      if re.match(f'(^{test.group}$)|(^{test.group}#[0-9]*$)', group_name):\n",
        "        match = re.search('#[0-9]*$', group_name)\n",
        "        group_num = max(group_num, int(match.group(0)[1:]) if match != None else 1)\n",
        "    group_dir = f'{log_base_route}/{test.group}#{group_num+1}'\n",
        "    last_group = test.group\n",
        "  test_num_dict[test.name] += 1\n",
        "  log_dir = f'{group_dir}/{test.name}#{test_num_dict[test.name]}'\n",
        "  # create complementary file writer\n",
        "  summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "  summary_writer.set_as_default()\n",
        "\n",
        "  tf.summary.text('test description', test.description(), 0)\n",
        "  start_time = datetime.utcnow()\n",
        "  start_time_CPU = time.process_time()\n",
        "  tf.summary.text('start time (in UTC)', str(start_time), 0)\n",
        "  tf.summary.flush()\n",
        "\n",
        "  # # # # # # # # #\n",
        "  # create  model #\n",
        "  # # # # # # # # #\n",
        "\n",
        "  # building the models\n",
        "  models = []\n",
        "  input_dim = test.input_dim\n",
        "  for _ in range(2): # build one model for PINN and one for the DRM\n",
        "    inputs = keras.Input(shape=(input_dim,), batch_size=None) # None means batch size is determined dynamically\n",
        "    x = layers.Dense(test.nodes_per_layer, activation=None if test.num_blocks > 0 else test.activation)(inputs) \n",
        "    #   in order to add the input via the skip connection to the output of the first res block, \n",
        "    #   the dimensions have to match. This can either be achieved via the linear transformation above\n",
        "    #   or (when dim input < dim res block) by padding zeros to the input\n",
        "    # x = tf.pad(inputs, [[0,0], [0,test.nodes_per_layer-test.input_dim]]) # pad the input with zeros until the new input dimension matches nodes_per_layer\n",
        "    #                                                                      # the first padding [0,0] is for the batch \"dimension\"\n",
        "    for k in range(test.num_blocks):\n",
        "      tmp = layers.Dense(test.nodes_per_layer, activation=test.activation)(x)\n",
        "      tmp = layers.Dense(test.nodes_per_layer, activation=test.activation)(tmp)\n",
        "      x = layers.add([tmp, x]) # this is the skip/residual connection\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    models += [keras.Model(inputs=inputs, outputs=outputs)]\n",
        "  model_PINN = models[0]\n",
        "  model_DRM = models[1]\n",
        "    # in order to guarantee the same initial conditions, \n",
        "    # copy the weights from the PINN model to the DRM model\n",
        "  model_DRM.set_weights(model_PINN.get_weights()) \n",
        "\n",
        "  # print model summary\n",
        "  # model_PINN.summary()\n",
        "  \n",
        "  # # # # # # # # # # # #\n",
        "  # training loop setup #\n",
        "  # # # # # # # # # # # #\n",
        "\n",
        "  opt = test.optimizer(test.learning_rate)\n",
        "  domain_params = test.domain_params\n",
        "\n",
        "  # generators\n",
        "  generate_interior = lambda: test.generate_interior(input_dim, *domain_params, *test.interior_params)\n",
        "  generate_interior_check = lambda: test.generate_interior_check(input_dim, *domain_params, *test.interior_check_params)\n",
        "  generate_boundary = lambda: test.generate_boundary(input_dim, *domain_params, *test.boundary_params)\n",
        "  generate_boundary_check = lambda: test.generate_boundary_check(input_dim, *domain_params, *test.boundary_check_params)\n",
        "\n",
        "  # rhs and exact solution\n",
        "  rhs = lambda X: test.rhs(*domain_params, X)\n",
        "  exact_sol = lambda X: test.exact_sol(*domain_params, X)\n",
        "\n",
        "  # train steps\n",
        "  train_step_PINN_acc = tf.function(train_step_PINN) # accelerate train step with tf.function\n",
        "  train_step_DRM_acc = tf.function(train_step_DRM) # accelerate train step with tf.function\n",
        "\n",
        "  # # # # # # # # # # # # # # # # # #\n",
        "  # train model using  PINN and DRM #\n",
        "  # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "  for epoch in range(test.epochs+1): # use range(epochs PLUS ONE) in order to finish at epoch == epochs\n",
        "    if epoch == 1: # save the \"setup\" time (time until one epoch is finished) this includes in particular the tracing time\n",
        "      tf.summary.text('setup duration', f'Wall time: {datetime.utcnow() - start_time}', 0)\n",
        "      tf.summary.text('setup duration', f'CPU time: {time.process_time() - start_time_CPU}', 1)\n",
        "      tf.summary.flush()\n",
        "\n",
        "    # determine if to log or print at this epoch\n",
        "    logging_on = epoch % test.logging_interval == 0 or epoch == test.epochs\n",
        "    printing_on = epoch % test.printing_interval == 0 or epoch == test.epochs\n",
        "    \n",
        "    # Q: why do we calc u_PINN and u_DRM before we perform the train step?\n",
        "    # A: the loss returned by train_step() is always the loss BEFORE the train step\n",
        "    #    in order for the metrics and loss to be in sync, we thus need to compute u_PINN and u_DRM before the train step\n",
        "    if logging_on or printing_on:\n",
        "      # calculate exact and approx solution and their derivatives in the interior needed for logging or printing\n",
        "      X_check, _ = generate_interior_check()\n",
        "      u_exact, udx_exact, u_PINN, udx_PINN, u_DRM, udx_DRM = calc_u_check(X_check, exact_sol, model_PINN, model_DRM)\n",
        "      # calculate exact and approx solution at the boundary needed for logging or printing\n",
        "      X_boundary_check, _ = generate_boundary_check()\n",
        "      u_boundary_exact, u_boundary_PINN, u_boundary_DRM = calc_u_boundary_check(X_boundary_check, exact_sol, model_PINN, model_DRM)\n",
        "\n",
        "    # execute one train step for PINN and the DRM\n",
        "    X_inner, V_inner = generate_interior()\n",
        "    f = rhs(X_inner)\n",
        "    X_boundary, V_boundary = generate_boundary()\n",
        "    g = exact_sol(X_boundary)\n",
        "    loss_PINN, loss_inner_PINN, loss_boundary_PINN = train_step_PINN_acc(X_inner, V_inner, f, X_boundary, V_boundary, g, model_PINN, test.beta, opt)\n",
        "    loss_DRM, loss_inner_DRM, loss_boundary_DRM = train_step_DRM_acc(X_inner, V_inner, f, X_boundary, V_boundary, g, model_DRM, test.beta, opt)\n",
        "\n",
        "    # printing to the console and logging\n",
        "    if logging_on or printing_on:\n",
        "      # log and print losses at the specified epoch\n",
        "      if logging_on:\n",
        "          tf.summary.scalar('PINN loss', loss_PINN, step=epoch)\n",
        "          tf.summary.scalar('PINN loss_inner', loss_inner_PINN, step=epoch)\n",
        "          tf.summary.scalar('PINN loss_boundary', loss_boundary_PINN, step=epoch)\n",
        "          tf.summary.scalar('DRM loss', loss_DRM, step=epoch)\n",
        "          tf.summary.scalar('DRM loss_inner', loss_inner_DRM, step=epoch)\n",
        "          tf.summary.scalar('DRM loss_boundary', loss_boundary_DRM, step=epoch)\n",
        "      if printing_on:\n",
        "        print(f\"loss at epoch {epoch}/{test.epochs}={math.floor(epoch/test.epochs*100)}%:\\tPINN {loss_PINN}\\tDRM {loss_DRM}\")\n",
        "\n",
        "      # loop over all metrics of interest to this test\n",
        "      for metric in test.metrics:\n",
        "        # calc metric for PINN\n",
        "          # inner error\n",
        "        metric.update_state(tf.stack([u_exact, *udx_exact]), tf.stack([u_PINN, *udx_PINN]))\n",
        "        metric_res_PINN = float(metric.result())\n",
        "        metric.reset_states()\n",
        "          # boundary error\n",
        "        metric.update_state(tf.stack([u_boundary_exact]), tf.stack([u_boundary_PINN]))\n",
        "        metric_res_boundary_PINN = float(metric.result())\n",
        "        metric.reset_states()\n",
        "\n",
        "        # calc metric for the DRM\n",
        "          # inner error\n",
        "        metric.update_state(tf.stack([u_exact, *udx_exact]), tf.stack([u_DRM, *udx_DRM]))\n",
        "        metric_res_DRM = float(metric.result())\n",
        "        metric.reset_states()\n",
        "          # boundary error\n",
        "        metric.update_state(tf.stack([u_boundary_exact]), tf.stack([u_boundary_DRM]))\n",
        "        metric_res_boundary_DRM = float(metric.result())\n",
        "        metric.reset_states()\n",
        "\n",
        "        # log and print metrics at the specified epoch\n",
        "        if logging_on:\n",
        "          tf.summary.scalar('PINN ' + metric.name, metric_res_PINN, step=epoch)\n",
        "          tf.summary.scalar('PINN ' + metric.name + ' boundary', metric_res_boundary_PINN, step=epoch)\n",
        "          tf.summary.scalar('DRM ' + metric.name, metric_res_DRM, step=epoch)\n",
        "          tf.summary.scalar('DRM ' + metric.name + ' boundary', metric_res_boundary_DRM, step=epoch)\n",
        "        if printing_on:\n",
        "          print(f\"{metric.name} at epoch {epoch}/{test.epochs}={math.floor(epoch/test.epochs*100)}%:\\tPINN {metric_res_PINN}\\tDRM {metric_res_DRM}\") \n",
        "      tf.summary.flush()\n",
        "  tf.summary.text('test duration', f'Wall time: {datetime.utcnow() - start_time}', 0)\n",
        "  tf.summary.text('test duration', f'CPU time: {time.process_time() - start_time_CPU} seconds', 1)\n",
        "  tf.summary.flush()\n",
        "  print(f'Wall time: {datetime.utcnow() - start_time}')\n",
        "  print(f'CPU time: {time.process_time() - start_time_CPU} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3r1hGr7hcl3"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training you can immediatly inspect your training data by \n",
        "opening it in the TensorBoard. The following cell will do that for all the tests found anywhere inside `log_base_route`. \n",
        "\n",
        "If the number of tests in this folder becomes to large you can specify a concrete path for example with \n",
        "`%tensorboard --logdir \"content/drive/MyDrive/PINN_vs_DRM_tests/logs/group/name\"` "
      ],
      "metadata": {
        "id": "vY-K9hYiyyan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1JYDpghI9V"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_base_route}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1A1nXQf0oaDt",
        "6uRuxvoHHsHq"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}